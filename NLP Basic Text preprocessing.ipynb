{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a188abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccaee895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\siddh\\\\OneDrive\\\\Desktop\\\\Data science data\\\\IMDB Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e14657e",
   "metadata": {},
   "source": [
    "## Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f9595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "807ef3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448180d4",
   "metadata": {},
   "source": [
    "## Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4047db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tags(data):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52653852",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454faee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wonderful little production. the filming technique is very unassuming- very old-time-bbc fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. the actors are extremely well chosen- michael sheen not only \"has got all the polari\" but he has all the voices down pat too! you can truly see the seamless editing guided by the references to williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. a masterful production about one of the great master\\'s of comedy and his life. the realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. it plays on our knowledge and our senses, particularly with the scenes concerning orton and halliwell and the sets (particularly of their flat with halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01821e5d",
   "metadata": {},
   "source": [
    "## Remove URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c30eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'google search here www.google.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b11815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8bb0b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google search here '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba501fef",
   "metadata": {},
   "source": [
    "## Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0030b6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punc = string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc97a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in punc:\n",
    "        text = text.replace(char,'')\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89f0986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster technique\n",
    "def remove_punc_fast(text):\n",
    "    return text.translate(str.maketrans('','',punc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a842ad55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = pd.read_csv(\"C:\\\\Users\\\\siddh\\\\OneDrive\\\\Desktop\\\\Data science data\\\\test_tweets_anuFYb8.csv\")\n",
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e12db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['tweet'] = tweet['tweet'].apply(remove_punc_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dbeafec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' user white supremacists want everyone to see the new 창\\x80\\x98  birds창\\x80\\x99 movie 창\\x80\\x94 and here창\\x80\\x99s why  '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet['tweet'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f349b1",
   "metadata": {},
   "source": [
    "## Chatword Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f67efd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"AFAIK\":\"As Far As I Know\",\n",
    "      \"AFK\":\"Away From Keyboard\",\n",
    "      \"ASAP\":\"As Soon As Possible\"}\n",
    "\n",
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in d:\n",
    "            new_text.append(d[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "            \n",
    "    return \" \".join(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc56b48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We have to reach there As Soon As Possible'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion(\"We have to reach there ASAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951b88f",
   "metadata": {},
   "source": [
    "## Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43dcef56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please read the notebook and write the notebook and return the notebook'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "incorrect_text = \"Please read the notebook and write the notbook amd return the ntebook\"\n",
    "textblb = TextBlob(incorrect_text)\n",
    "textblb.correct().string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed13fe9b",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "222bd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_sw(text):\n",
    "    new_text = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "            \n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afaf149b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probability  all-time favourite movie ,  story  selflessness,sacrifice  dedication'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_sw(\"probability my all-time favourite movie , a story of selflessness,sacrifice and dedication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325c3ca",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652840ed",
   "metadata": {},
   "source": [
    "#### Split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccda2bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "sent1 = \"I am going to delhi\"\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55328152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi',\n",
       " ' I will stay there for 3 days',\n",
       " \" Let's hope the trip to be great\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sent2 = \"I am going to delhi. I will stay there for 3 days. Let's hope the trip to be great\"\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0dfab",
   "metadata": {},
   "source": [
    "#### Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b26b7291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent3 = 'I am going to delhi!'\n",
    "tokens = re.findall(\"[\\w']+\",sent3)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fd6ff7",
   "metadata": {},
   "source": [
    "#### nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f306c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f153ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = 'I am going to visit delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "166eee9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '10.50']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2 = 'A 5km ride cost $10.50'\n",
    "word_tokenize(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da349a",
   "metadata": {},
   "source": [
    "#### spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1bb8e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6db37276",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53fbc258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "10.50\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea80874",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07142df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "564083b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af05984a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'walk walks walking walked'\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94768527",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ba3e70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "the                 the                 \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "a                   a                   \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "sun                 sun                 \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "sent = 'He was running and eating at the same time. He has a bad habit of swimming after playing long hours in sun'\n",
    "pun = '?:!.,;'\n",
    "\n",
    "sent_words = nltk.word_tokenize(sent)\n",
    "\n",
    "for word in sent_words:\n",
    "    if word in punc:\n",
    "        sent_words.remove(word)\n",
    "        \n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "\n",
    "for word in sent_words:\n",
    "    print(\"{0:20}{1:20}\".format(word,wl.lemmatize(word,pos='v')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b54b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
